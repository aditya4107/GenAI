{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_channels, imsize):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # in_filters, out_filters, kernel, stride, padding\n",
    "            nn.Conv2d(n_channels,  imsize * 4, 4, 2, 1), # conv1 64x64\n",
    "            nn.BatchNorm2d(imsize * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(imsize * 4, imsize * 8, kernel_size=4, # conv2 I=32x32\n",
    "                      stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(imsize * 8, imsize * 8, kernel_size=4, # conv3 I=16x16\n",
    "                      stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(imsize * 8, imsize * 8, kernel_size=4, # conv4 O=2x2\n",
    "                      stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(imsize * 8, imsize * 8, kernel_size=4, # conv5\n",
    "                      stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # conv6\n",
    "            nn.Conv2d(imsize * 8, 1, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, n_channels, imsize):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_size, imsize * 8, kernel_size=4, #conv1\n",
    "                               stride=1, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(imsize * 8, imsize * 8, kernel_size=4, #conv2\n",
    "                               stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(imsize * 8, imsize * 8, kernel_size=4, #conv3\n",
    "                               stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(imsize * 8, imsize * 4, kernel_size=4, #conv4\n",
    "                               stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(imsize * 4, imsize * 2,kernel_size=4,#conv5\n",
    "                               stride=2,padding=1),\n",
    "            nn.BatchNorm2d(imsize * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(imsize * 2, n_channels, kernel_size=4,#conv6\n",
    "                               stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "\n",
    "            )\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, device, save_dir):\n",
    "        self.generator = None\n",
    "        self.discriminator = None\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "\n",
    "    def compile(self, latent_size=100, n_channels=3, imsize=64):\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.generator = Generator(latent_size, n_channels, imsize).to(self.device)\n",
    "        self.discriminator = Discriminator(n_channels, imsize).to(self.device)\n",
    "        dis_learning_rate = 0.0002\n",
    "        gen_learning_rate = 0.0002\n",
    "        self.dis_optimizer = optim.Adam(self.discriminator.parameters(),\n",
    "                           dis_learning_rate, betas=(0.5, 0.999))\n",
    "        self.gen_optimizer = optim.Adam(self.generator.parameters(),\n",
    "                           dis_learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    def load(self, save_dir):\n",
    "        self.generator.load_state_dict(torch.load(\n",
    "                join(save_dir, \"generator_state.pt\")))\n",
    "        self.discriminator.load_state_dict(torch.load(\n",
    "                join(save_dir, \"discriminator_state.pt\")))\n",
    "        self.discriminator.eval()\n",
    "        self.generator.eval()\n",
    "\n",
    "    def  __call__(self, input):\n",
    "        self.discriminator.eval()\n",
    "        self.generator.eval()\n",
    "        return self.generator(input)\n",
    "\n",
    "    def train(self, dataloader, n_epochs):\n",
    "\n",
    "        if self.generator is None:\n",
    "            raise Exception(\"Compile the model before training\")\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # keep track of progress\n",
    "        self.G_losses = []\n",
    "        self.D_losses = []\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            for i, data in enumerate(dataloader, 0):\n",
    "                # load image batch\n",
    "                image, _ = data\n",
    "                image = image.to(self.device)\n",
    "                batch_size = image.size()[0]\n",
    "                trues = torch.ones([batch_size, 1], device=self.device)\n",
    "                fakes = torch.zeros([batch_size, 1], device=self.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # TRAIN DISCRIMINATIVE\n",
    "                # zero grads\n",
    "                self.dis_optimizer.zero_grad()\n",
    "                # true batch\n",
    "                out = self.discriminator(image).view(-1, 1)\n",
    "                loss_real = criterion(out, trues)\n",
    "                loss_real.backward(retain_graph=True)\n",
    "                accuracy_dis = out.mean().item()\n",
    "\n",
    "                # fake batch\n",
    "                noise = torch.randn(batch_size, self.latent_size, 1, 1, device=self.device)\n",
    "\n",
    "                fake = self.generator(noise)\n",
    "                out = self.discriminator(fake).view(-1, 1)\n",
    "                loss_fake = criterion(out, fakes)\n",
    "                loss_fake.backward(retain_graph=True)\n",
    "                lossD = loss_real + loss_fake\n",
    "                self.dis_optimizer.step()\n",
    "                accuracy_gen_1 = out.mean().item()\n",
    "\n",
    "                # TRAIN GENERATOR\n",
    "                # zero grads\n",
    "                self.gen_optimizer.zero_grad()\n",
    "\n",
    "                out = self.discriminator(fake).view(-1, 1)\n",
    "                lossG = criterion(out, trues)\n",
    "                lossG.backward()\n",
    "                self.gen_optimizer.step()\n",
    "                accuracy_gen_2 = out.mean().item()\n",
    "\n",
    "                # PRINT STATS\n",
    "                if i % 100 == 0:\n",
    "                    torch.save(self.discriminator.state_dict(),\n",
    "                               join(self.save_dir, \"discriminator_state.pt\"))\n",
    "                    torch.save(self.generator.state_dict(),\n",
    "                               join(self.save_dir, \"generator_state.pt\"))\n",
    "                    print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tAcc_dis: %.4f\\tAcc_gen: %.4f / %.4f'\n",
    "                  % (epoch, n_epochs, i, len(dataloader),\n",
    "                     lossD.item(), lossG.item(), accuracy_dis, accuracy_gen_1, accuracy_gen_2))\n",
    "\n",
    "                self.G_losses.append(lossG.item())\n",
    "                self.D_losses.append(lossD.item())\n",
    "\n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "        plt.plot(self.G_losses,label=\"G\")\n",
    "        plt.plot(self.D_losses,label=\"D\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os \n",
    "\n",
    "# device gpu device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "imsize = 64\n",
    "n_channels = 3\n",
    "latent_size = 100\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root=\"resized_CelebA/\" , transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os \n",
    "\n",
    "# device gpu device\n",
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "imsize = 64\n",
    "n_channels = 3\n",
    "latent_size = 100\n",
    "\n",
    "save_dir = \"models/GAN/model_states/\"\n",
    "\n",
    "\n",
    "model = GAN(device=device, save_dir=save_dir)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    n_epochs = 25\n",
    "    model.train(dataloader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from file \n",
    "load_dir = \"models/GAN/model_states/version_0_10e/\"\n",
    "model.load(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(img, figsize=(16,16)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    img_list = [vutils.make_grid(img, padding=2, normalize=True).cpu().detach()]\n",
    "\n",
    "    ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(32, 100,1, 1, device=device)\n",
    "img = model(noise)\n",
    "display_images(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write an encoder to learn the input image to latent vector mapping for this generator gan\n",
    "# use the encoder to generate latent vectors for the images in the dataset\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size, n_channels, imsize):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(n_channels,  imsize * 4, 4, 2, 1), # conv1 64x64\n",
    "            nn.BatchNorm2d(imsize * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(imsize * 4, imsize * 8, kernel_size=4, # conv2 I=32x32\n",
    "                      stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(imsize * 8, imsize * 8, kernel_size=4, # conv3 I=16x16\n",
    "                      stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(imsize * 8, imsize * 8, kernel_size=4, # conv4 O=2x2\n",
    "                      stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(imsize * 8, imsize * 8, kernel_size=4, # conv5\n",
    "                      stride=2, padding=1),\n",
    "            nn.BatchNorm2d(imsize * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # conv6\n",
    "            nn.Conv2d(imsize * 8, latent_size, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "    \n",
    "\n",
    "\n",
    "def train_encoder(encoder, generator, dataloader, n_epochs):\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # load image batch\n",
    "            image, _ = data\n",
    "            image = image.to(device)\n",
    "            batch_size = image.size()[0]\n",
    "\n",
    "            # zero grads\n",
    "            encoder_optimizer.zero_grad()\n",
    "\n",
    "            # true batch\n",
    "            latent = encoder(image)\n",
    "            fake = generator(latent)\n",
    "            loss = criterion(fake, image)\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('[%d/%d][%d/%d]\\tLoss: %.4f'\n",
    "              % (epoch, n_epochs, i, len(dataloader), loss.item()))\n",
    "\n",
    "    return encoder\n",
    "\n",
    "if train:\n",
    "    encoder = Encoder(latent_size, n_channels, imsize).to(device)\n",
    "    encoder = train_encoder(encoder, model.generator, dataloader, 5)\n",
    "    torch.save(encoder.state_dict(), join(save_dir, \"encoder_state.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encoder and train more\n",
    "\n",
    "encoder = Encoder(latent_size, n_channels, imsize).to(device)\n",
    "encoder.load_state_dict(torch.load(join(save_dir, \"encoder_state_new.pt\")))\n",
    "\n",
    "if train:\n",
    "    encoder = train_encoder(encoder, model.generator, dataloader, 5)\n",
    "    torch.save(encoder.state_dict(), join(save_dir, \"encoder_state_new.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder.state_dict(), join(save_dir, \"encoder_state.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     a = np.random.randint(0, len(dataset))\n",
    "#     test_img = dataset[a][0].unsqueeze(0).to(device)\n",
    "#     latent = encoder(test_img)\n",
    "#     img = model.generator(latent)\n",
    "\n",
    "#     # plot test_img and img\n",
    "#     display_images(test_img , figsize=(2,2))\n",
    "#     display_images(img , figsize=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 202599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_attr_original = pd.read_csv(\"list_attr_celeba.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(len(img_attr_original), n, replace=False)\n",
    "img_attr = img_attr_original.iloc[random_indices]\n",
    "img_attr.drop(\"Unnamed: 41\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start = 162769\n",
    "test_start = 182636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_attr = img_attr[img_attr.index >= test_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_smiling = []\n",
    "people_hat = [] \n",
    "people_no_hat = []\n",
    "people_mustache = []\n",
    "people_no_mustache = []\n",
    "\n",
    "\n",
    "\n",
    "chosen = 0\n",
    "while chosen < 3:\n",
    "    a = np.random.randint(0, len(test_img_attr))\n",
    "    row = test_img_attr.iloc[a]\n",
    "    if row[\"Smiling\"] == 1 and row[\"Male\"] == 1:\n",
    "        men_smiling.append(row[\"Serial\"])\n",
    "        # print(row[\"Serial\"])\n",
    "        # print(a)\n",
    "        chosen += 1\n",
    "\n",
    "men_smiling_ix = [int(i[:-4])-1 for i in men_smiling]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chosen = 0\n",
    "while chosen < 3:\n",
    "    a = np.random.randint(0, len(test_img_attr))\n",
    "    row = test_img_attr.iloc[a]\n",
    "    if row[\"Wearing_Hat\"] == 1 :#and row[\"Male\"] == -1:\n",
    "        people_hat.append(row[\"Serial\"])\n",
    "        # print(row[\"Serial\"])\n",
    "        # print(a)\n",
    "        chosen += 1\n",
    "\n",
    "people_hat_ix = [int(i[:-4])-1 for i in people_hat]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chosen = 0\n",
    "while chosen < 3:\n",
    "    a = np.random.randint(0, len(test_img_attr))\n",
    "    row = test_img_attr.iloc[a]\n",
    "    if row[\"Wearing_Hat\"] == -1 :#and row[\"Male\"] == -1:\n",
    "        people_no_hat.append(row[\"Serial\"])\n",
    "        # print(row[\"Serial\"])\n",
    "        # print(a)\n",
    "        chosen += 1\n",
    "\n",
    "people_no_hat_ix = [int(i[:-4])-1 for i in people_no_hat]\n",
    "\n",
    "\n",
    "\n",
    "chosen = 0\n",
    "while chosen < 3:\n",
    "    a = np.random.randint(0, len(test_img_attr))\n",
    "    row = test_img_attr.iloc[a]\n",
    "    if row[\"Mustache\"] == 1 :#and row[\"Male\"] == -1:\n",
    "        people_mustache.append(row[\"Serial\"])\n",
    "        # print(row[\"Serial\"])\n",
    "        # print(a)\n",
    "        chosen += 1\n",
    "\n",
    "people_mustache_ix = [int(i[:-4])-1 for i in people_mustache]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chosen = 0\n",
    "while chosen < 3:\n",
    "    a = np.random.randint(0, len(test_img_attr))\n",
    "    row = test_img_attr.iloc[a]\n",
    "    if row[\"Mustache\"] == -1 :#and row[\"Male\"] == -1:\n",
    "        people_no_mustache.append(row[\"Serial\"])\n",
    "        # print(row[\"Serial\"])\n",
    "        # print(a)\n",
    "        chosen += 1\n",
    "\n",
    "people_no_mustache_ix = [int(i[:-4])-1 for i in people_no_mustache]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgtemp = []\n",
    "for i in men_smiling_ix:\n",
    "    imgtemp.append(dataset[i][0].unsqueeze(0).to(device))\n",
    "\n",
    "display_img_ms = torch.cat((imgtemp[0] , imgtemp[1] , imgtemp[2]), dim = 0)\n",
    "display_images(display_img_ms , figsize=(9,9))\n",
    "\n",
    "\n",
    "imgtemp = []\n",
    "for i in people_hat_ix:\n",
    "    imgtemp.append(dataset[i][0].unsqueeze(0).to(device))\n",
    "\n",
    "display_img_ph = torch.cat((imgtemp[0] , imgtemp[1] , imgtemp[2]), dim = 0)\n",
    "display_images(display_img_ph , figsize=(9,9))\n",
    "\n",
    "\n",
    "imgtemp = []\n",
    "for i in people_no_hat_ix:\n",
    "    imgtemp.append(dataset[i][0].unsqueeze(0).to(device))\n",
    "\n",
    "display_img_pnh = torch.cat((imgtemp[0] , imgtemp[1] , imgtemp[2]), dim = 0)\n",
    "display_images(display_img_pnh , figsize=(9,9))\n",
    "\n",
    "\n",
    "imgtemp = []\n",
    "for i in people_mustache_ix:\n",
    "    imgtemp.append(dataset[i][0].unsqueeze(0).to(device))\n",
    "\n",
    "display_img_pm = torch.cat((imgtemp[0] , imgtemp[1] , imgtemp[2]), dim = 0)\n",
    "display_images(display_img_pm , figsize=(9,9))\n",
    "\n",
    "imgtemp = []\n",
    "for i in people_no_mustache_ix:\n",
    "    imgtemp.append(dataset[i][0].unsqueeze(0).to(device))\n",
    "\n",
    "display_img_pnm = torch.cat((imgtemp[0] , imgtemp[1] , imgtemp[2]), dim = 0)\n",
    "display_images(display_img_pnm , figsize=(9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_mean = (display_img_ms[0] + display_img_ms[1] + display_img_ms[2]) / 3\n",
    "ph_mean = (display_img_ph[0] + display_img_ph[1] + display_img_ph[2]) / 3\n",
    "pnh_mean = (display_img_pnh[0] + display_img_pnh[1] + display_img_pnh[2]) / 3\n",
    "pm_mean = (display_img_pm[0] + display_img_pm[1] + display_img_pm[2]) / 3\n",
    "pnm_mean = (display_img_pnm[0] + display_img_pnm[1] + display_img_pnm[2]) / 3\n",
    "\n",
    "disp = torch.cat((ms_mean.unsqueeze(0) , ph_mean.unsqueeze(0) , pnh_mean.unsqueeze(0) , pm_mean.unsqueeze(0) , pnm_mean.unsqueeze(0)), dim = 0)\n",
    "\n",
    "display_images(disp , figsize=(10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = (encoder(display_img_pnm))\n",
    "# b = a[0] + a[1] + a[2]\n",
    "# display_images(model(b.view(-1 , 100 , 1 , 1)).view(-1 , 3, 64, 64) , figsize=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_mean = (ms_mean + ph_mean - pnh_mean + pm_mean - pnm_mean)\n",
    "display_images(tot_mean.unsqueeze(0) , figsize=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_img_ms_latent = encoder(display_img_ms)\n",
    "display_img_ph_latent = encoder(display_img_ph)\n",
    "display_img_pnh_latent = encoder(display_img_pnh)\n",
    "display_img_pm_latent = encoder(display_img_pm)\n",
    "display_img_pnm_latent = encoder(display_img_pnm)\n",
    "\n",
    "display_img_ms_latent_mean = (display_img_ms_latent[0] + display_img_ms_latent[1] + display_img_ms_latent[2]) / 3\n",
    "display_img_ph_latent_mean = (display_img_ph_latent[0] + display_img_ph_latent[1] + display_img_ph_latent[2]) / 3\n",
    "display_img_pnh_latent_mean = (display_img_pnh_latent[0] + display_img_pnh_latent[1] + display_img_pnh_latent[2]) / 3\n",
    "display_img_pm_latent_mean = (display_img_pm_latent[0] + display_img_pm_latent[1] + display_img_pm_latent[2]) / 3\n",
    "display_img_pnm_latent_mean = (display_img_pnm_latent[0] + display_img_pnm_latent[1] + display_img_pnm_latent[2]) / 3\n",
    "\n",
    "disp_latent = torch.cat((display_img_ms_latent_mean.unsqueeze(0) , display_img_ph_latent_mean.unsqueeze(0) , display_img_pnh_latent_mean.unsqueeze(0) , display_img_pm_latent_mean.unsqueeze(0) , display_img_pnm_latent_mean.unsqueeze(0)), dim = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(model(disp_latent) , figsize=(10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_disp_latent = display_img_ms_latent_mean + display_img_ph_latent_mean - display_img_pnh_latent_mean + display_img_pm_latent_mean - display_img_pnm_latent_mean\n",
    "final_disp = model(final_disp_latent.view(-1 , 100, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(8 , 100, 1, 1) * 0.25\n",
    "noise = torch.cat((noise , torch.zeros(1 , 100, 1 , 1)) , dim = 0).to(device)\n",
    "\n",
    "final_disp_grid = torch.cat(tuple([final_disp_latent for i in range(9)]) , dim = 0).view(-1 , 100 , 1 , 1)\n",
    "\n",
    "final_disp_grid = final_disp_grid + noise\n",
    "\n",
    "final_disp_grid = model(final_disp_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(model(disp_latent) , figsize=(10,2))\n",
    "display_images(final_disp_grid[:3] , figsize=(5,5))\n",
    "display_images(final_disp_grid[3:6] , figsize=(5,5))\n",
    "display_images(final_disp_grid[6:] , figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# men_neutral_latent = encoder(display_img_mn)\n",
    "# women_neutral_latent = encoder(display_img_wn)\n",
    "# women_smiling_latent = encoder(display_img_ws)\n",
    "\n",
    "# men_neutral_mean = sum(men_neutral_latent[i] for i in range(3))/3\n",
    "# women_neutral_mean = sum(women_neutral_latent[i] for i in range(3))/3\n",
    "# women_smiling_mean = sum(women_smiling_latent[i] for i in range(3))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"-------------------------Images--------------------------------------\" , \"-------mean--------\")\n",
    "\n",
    "# dis_wn = torch.cat( (display_img_wn, model(women_neutral_mean.view(-1,100,1,1)) ), dim = 0)\n",
    "# dis_ws = torch.cat( (display_img_ws , model(women_smiling_mean.view(-1 , 100, 1 ,1))), dim = 0)\n",
    "# dis_mn = torch.cat( (display_img_mn , model(men_neutral_mean.view(-1 , 100, 1 ,1))), dim = 0)\n",
    "# display_images(dis_ws , figsize=(9,9))\n",
    "# # print(\"Women Neutral\")\n",
    "# display_images(dis_mn , figsize=(9,9))\n",
    "# # print(\"Women smiling\")\n",
    "# display_images(dis_wn , figsize=(9,9))\n",
    "# # print(\"Men Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"-------------------------Images--------------------------------------\" , \"-------mean--------\")\n",
    "# mean_wn = (display_img_wn[0] + display_img_wn[1] + display_img_wn[2])/3\n",
    "# mean_ws = (display_img_ws[0] + display_img_ws[1] + display_img_ws[2])/3\n",
    "# mean_mn = (display_img_mn[0] + display_img_mn[1] + display_img_mn[2])/3\n",
    "\n",
    "\n",
    "# dis_wn = torch.cat( (display_img_wn, mean_wn.view(-1,3,64,64) ), dim = 0)\n",
    "# dis_ws = torch.cat( (display_img_ws , mean_ws.view(-1,3,64,64)), dim = 0)\n",
    "# dis_mn = torch.cat( (display_img_mn , mean_mn.view(-1,3,64,64)), dim = 0)\n",
    "# display_images(dis_ws , figsize=(9,9))\n",
    "# # print(\"Women Neutral\")\n",
    "# display_images(dis_wn , figsize=(9,9))\n",
    "# # print(\"Women smiling\")\n",
    "# display_images(dis_mn , figsize=(9,9))\n",
    "# # print(\"Men Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = mean_wn + mean_ws - mean_mn\n",
    "# display_images(total , figsize=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc = torch.cat((dis_ws[-1] , dis_mn[-1] , dis_wn[-1]) , dim = 0)\n",
    "# display_images(abc.view(3,3,64,64) , figsize=(9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# man_smiling =  women_smiling_mean + women_neutral_mean - men_neutral_mean  \n",
    "# # man_smiling_img = model(man_smiling.view(-1,100,1,1))\n",
    "\n",
    "# man_smiling_9grid = torch.cat(tuple([man_smiling for i in range(9)])).view(9, 100, 1, 1)\n",
    "\n",
    "# noise = torch.randn(8, 100,1, 1, device=device) * 0.25\n",
    "# noise = torch.cat((noise, torch.zeros(1, 100, 1,1, device = device)))\n",
    "\n",
    "# man_smiling_9grid = man_smiling_9grid + noise\n",
    "\n",
    "# disp_now = model(man_smiling_9grid.view(-1,100,1,1))\n",
    "\n",
    "# display_images(disp_now[:3], figsize = (5,5))\n",
    "# display_images(disp_now[3:6], figsize = (5,5))\n",
    "# display_images(disp_now[6:], figsize = (5,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
